{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudscraper in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (1.2.71)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from cloudscraper) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.9.2 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from cloudscraper) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from cloudscraper) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from selenium) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\general\\itc\\i5-gic\\semester-1\\nlp\\mini project 1\\nlp_mini_project\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install beautifulsoup4\n",
    "%pip install lxml\n",
    "%pip install cloudscraper\n",
    "%pip install selenium\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved to cleaned_wiki_text.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def clean_wikipedia_text(text):\n",
    "    # Remove citations like [1], [2][3], [citation needed]\n",
    "    text = re.sub(r'\\[\\d+(?:,\\d+)*\\]|\\[citation needed\\]', '', text)\n",
    "    \n",
    "    # Correct incorrectly escaped HTML entities like `&lt;` and `&gt;`\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # Remove parenthetical information\n",
    "    text = re.sub(r'\\([^)]*\\)-', '', text)\n",
    "    \n",
    "    # Remove special formatting (e.g., ''italic'', \"\"\"bold\"\"\" or similar)\n",
    "    text = re.sub(r\"''+|\\\"\\\"+|'''\", '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove lines starting with special characters (-, *, digits, or #)\n",
    "    text = '\\n'.join(\n",
    "        line for line in text.splitlines()\n",
    "        if not re.match(r'^(\\s*[-*#]|\\s*\\d+\\.)', line.strip())\n",
    "    )\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs('data_clean', exist_ok=True)\n",
    "\n",
    "# Load text, clean, and save the output\n",
    "input_file = 'data.txt'\n",
    "output_file = 'cleaned_wiki_text.txt'\n",
    "\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        raw_text = file.read()\n",
    "    \n",
    "    cleaned_text = clean_wikipedia_text(raw_text)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "    \n",
    "    print(f\"Cleaned text saved to {output_file}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{input_file}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149839\n"
     ]
    }
   ],
   "source": [
    "with open('cleaned_wiki_text.txt', 'r', encoding='utf-8') as file:\n",
    "    cleaned_text = file.read()\n",
    "    \n",
    "print(len(cleaned_text.strip(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split text to three part\n",
    "- 70% for train\n",
    "- 10% for validation\n",
    "- 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and testing sets saved to 'training_sentences.txt', 'validation_sentences.txt', and 'testing_sentences.txt' respectively.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load text from file\n",
    "with open('cleaned_wiki_text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()\n",
    "    \n",
    "text = re.sub(r'\\n+', '', text)\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text = re.sub(r'\\[\\]', '', text)\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = text.split('.')\n",
    "# print(len(sentences))\n",
    "\n",
    "# Shuffle sentences\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "training_sentences = sentences[:int(0.8 * len(sentences))]\n",
    "validation_sentences = sentences[int(0.8 * len(sentences)):int(0.9 * len(sentences))]\n",
    "testing_sentences = sentences[int(0.9 * len(sentences)):]\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "# Write to files\n",
    "with open('training_data.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(''.join(training_sentences))\n",
    "\n",
    "with open('validation_data.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(''.join(validation_sentences))\n",
    "\n",
    "with open('sting_data.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(''.join(testing_sentences))\n",
    "\n",
    "print(\"Training, validation, and testing sets saved to 'training_sentences.txt', 'validation_sentences.txt', and 'testing_sentences.txt' respectively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram counts saved to 'LM1_ngram_counts' directory.\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Count n-grams\n",
    "def count_ngrams(tokens, n):\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for text in tokens:\n",
    "        for ngram in ngrams(text, n, pad_left=True, pad_right=True):\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Backoff probability computation\n",
    "def backoff_probability(ngram, ngram_counts, lower_ngram_counts):\n",
    "    if ngram_counts[ngram] > 0:\n",
    "        return ngram_counts[ngram] / lower_ngram_counts[ngram[:-1]]\n",
    "    elif len(ngram) > 1:\n",
    "        return backoff_probability(ngram[1:], lower_ngram_counts, unigram_counts)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# Save n-gram counts to a file\n",
    "def save_ngram_counts(ngram_counts, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write the header\n",
    "        writer.writerow(['word', 'count'])\n",
    "        \n",
    "        # Write the n-gram counts\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            ngram_str = ' '.join(str(token) for token in ngram if token is not None)\n",
    "            writer.writerow([ngram_str, count])\n",
    "# def save_ngram_counts(ngram_counts, output_file):\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         for ngram, count in ngram_counts.items():\n",
    "#             ngram_str = ' '.join(str(token) for token in ngram if token is not None)\n",
    "#             f.write(f\"{ngram_str}\\t{count}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the tokenized data directory exists\n",
    "    tokenized_data_dir = \"./\"\n",
    "    if not os.path.exists(tokenized_data_dir):\n",
    "        raise FileNotFoundError(f\"Directory '{tokenized_data_dir}' not found. Run the tokenization script first.\")\n",
    "\n",
    "    # Load tokenized training data\n",
    "    train_tokens = []\n",
    "    with open(os.path.join(tokenized_data_dir, \"training_data.txt\"), \"r\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            train_tokens.append(line.strip().split())\n",
    "\n",
    "    # Count n-grams\n",
    "    unigram_counts = count_ngrams(train_tokens, 1)\n",
    "    bigram_counts = count_ngrams(train_tokens, 2)\n",
    "    trigram_counts = count_ngrams(train_tokens, 3)\n",
    "    fourgram_counts = count_ngrams(train_tokens, 4)\n",
    "\n",
    "    # Save n-gram counts to files\n",
    "    output_dir = \"LM1_ngram_counts\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_ngram_counts(unigram_counts, os.path.join(output_dir, \"unigram_counts.csv\"))\n",
    "    save_ngram_counts(bigram_counts, os.path.join(output_dir, \"bigram_counts.csv\"))\n",
    "    save_ngram_counts(trigram_counts, os.path.join(output_dir, \"trigram_counts.csv\"))\n",
    "    save_ngram_counts(fourgram_counts, os.path.join(output_dir, \"fourgram_counts.csv\"))\n",
    "\n",
    "    print(\"N-gram counts saved to 'LM1_ngram_counts' directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
